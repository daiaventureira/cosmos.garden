{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import contractions\n",
    "import pickle\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator, pad_sequences\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Activation, Embedding, LSTM, Dropout, GRU, Bidirectional, Input, RepeatVector, TimeDistributed\n",
    "from tensorflow.keras.callbacks import CSVLogger, EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "import os\n",
    "from functools import reduce\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "from datetime import date, datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(424242)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# GPU testing\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "print(tf.test.is_built_with_cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./Documents/scripts/cleaned_corpora.csv')\n",
    "\n",
    "# removes punctuation for simplicity\n",
    "df.message = df.message.apply(contractions.fix)\n",
    "df.message = df.message.apply(lambda m: re.sub(r\"[\\d:\\.\\?!,]\", '', m).lower())\n",
    "df.message = df.message.apply(lambda m: m.replace(\"'\", ''))\n",
    "df.message = df.message.apply(lambda m: re.sub(r\" +\", ' ', m))\n",
    "\n",
    "df = df[df.message.apply(lambda m: len(m.split(' '))) >= 40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', '-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "alphabet = df.message.tolist()\n",
    "alphabet = map(set, alphabet)\n",
    "alphabet = reduce(lambda s, u: s.union(u), alphabet, set([]))\n",
    "\n",
    "print(sorted(alphabet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = pd.to_datetime(df.date)\n",
    "\n",
    "YEARS = df.date.apply(lambda d: d.year).unique()\n",
    "\n",
    "year_std = np.std(YEARS)\n",
    "year_mean = np.mean(YEARS)\n",
    "\n",
    "df['year'] = df.date.apply(lambda d: (d.year - year_mean) / year_std)\n",
    "\n",
    "df['cos_month'] = df.date.apply(lambda d: np.cos(np.pi * 2 * d.month / 12))\n",
    "df['sin_month'] = df.date.apply(lambda d: np.sin(np.pi * 2 * d.month / 12))\n",
    "\n",
    "df['cos_day'] = df.date.apply(lambda d: np.cos(np.pi * 2 * d.day / 31))\n",
    "df['sin_day'] = df.date.apply(lambda d: np.sin(np.pi * 2 * d.day / 31))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sign_dummies = pd.get_dummies(df.star_sign_index, prefix='star_sign')\n",
    "df[sign_dummies.columns] = sign_dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>message</th>\n",
       "      <th>source</th>\n",
       "      <th>star_sign_index</th>\n",
       "      <th>star_sign_name</th>\n",
       "      <th>year</th>\n",
       "      <th>cos_month</th>\n",
       "      <th>sin_month</th>\n",
       "      <th>cos_day</th>\n",
       "      <th>sin_day</th>\n",
       "      <th>...</th>\n",
       "      <th>star_sign_2</th>\n",
       "      <th>star_sign_3</th>\n",
       "      <th>star_sign_4</th>\n",
       "      <th>star_sign_5</th>\n",
       "      <th>star_sign_6</th>\n",
       "      <th>star_sign_7</th>\n",
       "      <th>star_sign_8</th>\n",
       "      <th>star_sign_9</th>\n",
       "      <th>star_sign_10</th>\n",
       "      <th>star_sign_11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-05-02</td>\n",
       "      <td>you may feel like you are being left out of th...</td>\n",
       "      <td>HoroscopeCom</td>\n",
       "      <td>0</td>\n",
       "      <td>Aries</td>\n",
       "      <td>1.303572</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.918958</td>\n",
       "      <td>0.394356</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-05-03</td>\n",
       "      <td>is a romantic partner having trouble communica...</td>\n",
       "      <td>HoroscopeCom</td>\n",
       "      <td>0</td>\n",
       "      <td>Aries</td>\n",
       "      <td>1.303572</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.820763</td>\n",
       "      <td>0.571268</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        date                                            message        source  \\\n",
       "0 2019-05-02  you may feel like you are being left out of th...  HoroscopeCom   \n",
       "1 2019-05-03  is a romantic partner having trouble communica...  HoroscopeCom   \n",
       "\n",
       "   star_sign_index star_sign_name      year  cos_month  sin_month   cos_day  \\\n",
       "0                0          Aries  1.303572  -0.866025        0.5  0.918958   \n",
       "1                0          Aries  1.303572  -0.866025        0.5  0.820763   \n",
       "\n",
       "    sin_day  ...  star_sign_2  star_sign_3  star_sign_4  star_sign_5  \\\n",
       "0  0.394356  ...            0            0            0            0   \n",
       "1  0.571268  ...            0            0            0            0   \n",
       "\n",
       "   star_sign_6  star_sign_7  star_sign_8  star_sign_9  star_sign_10  \\\n",
       "0            0            0            0            0             0   \n",
       "1            0            0            0            0             0   \n",
       "\n",
       "   star_sign_11  \n",
       "0             0  \n",
       "1             0  \n",
       "\n",
       "[2 rows x 22 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_SIZE = 0.9\n",
    "\n",
    "X_len = df.shape[0]\n",
    "X_train_len = int(X_len * TRAINING_SIZE)\n",
    "\n",
    "idx = np.random.random(df.shape[0]) < TRAINING_SIZE\n",
    "\n",
    "X_train = df[idx]\n",
    "X_test = df[~idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train (42400, 22)\n",
      "X_test (4702, 22)\n"
     ]
    }
   ],
   "source": [
    "print('X_train', X_train.shape)\n",
    "print('X_test', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = dict()\n",
    "GLOVE_SIZE = 200\n",
    "\n",
    "with open('./Documents/scripts/glove/glove.6B.{}d.txt'.format(GLOVE_SIZE), encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 50000\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(X_train.message.values)\n",
    "sequences = tokenizer.texts_to_sequences(X_train.message.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCABULARY_SIZE = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_INIT = np.zeros((VOCABULARY_SIZE, GLOVE_SIZE))\n",
    "\n",
    "for (word, index) in tokenizer.word_index.items():\n",
    "    if word not in embeddings_index:\n",
    "        continue\n",
    "        \n",
    "    EMBEDDING_INIT[index] = embeddings_index[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "STATIC_COLUMNS = [\n",
    "    'year',\n",
    "    \n",
    "    'cos_month',\n",
    "    'sin_month',\n",
    "    \n",
    "    'cos_day',\n",
    "    'sin_day',\n",
    "    \n",
    "    'star_sign_0', \n",
    "    'star_sign_1', \n",
    "    'star_sign_2', \n",
    "    'star_sign_3', \n",
    "    'star_sign_4',\n",
    "    'star_sign_5', \n",
    "    'star_sign_6', \n",
    "    'star_sign_7', \n",
    "    'star_sign_8',\n",
    "    'star_sign_9', \n",
    "    'star_sign_10', \n",
    "    'star_sign_11',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENS_BEFORE_PREDICTION = 30\n",
    "TOKENS_PREDICTED = 1\n",
    "STRIDE = TOKENS_BEFORE_PREDICTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sequence_sets(X):\n",
    "    X_ = X.copy()\n",
    "    \n",
    "    seqs = np.array(tokenizer.texts_to_sequences(X.message.values)).reshape(-1, 1)\n",
    "    X_ = X_[STATIC_COLUMNS].values\n",
    "    \n",
    "    X_ = np.concatenate([X_, seqs], axis=1)\n",
    "    \n",
    "    def slide_window(xs):\n",
    "        return np.asarray([\n",
    "            np.concatenate([\n",
    "                xs[:-1], # static feats\n",
    "                xs[-1][i * STRIDE : i * STRIDE + TOKENS_BEFORE_PREDICTION] #text\n",
    "            ])\n",
    "            for i in range(((len(xs[-1]) - TOKENS_BEFORE_PREDICTION) // STRIDE) + 1)\n",
    "        ])\n",
    "\n",
    "    X_ = np.asarray(list(map(slide_window, X_)))\n",
    "    X_ = np.vstack(X_)\n",
    "    \n",
    "    sequences = X_[:, len(STATIC_COLUMNS):].astype(int)\n",
    "    statics = X_[:, :len(STATIC_COLUMNS)].astype(float)\n",
    "    \n",
    "    y = sequences[:, TOKENS_PREDICTED:]\n",
    "    sequences = sequences[:, :-TOKENS_PREDICTED]\n",
    "\n",
    "#     y = tf.keras.utils.to_categorical(y, num_classes=VOCABULARY_SIZE, dtype='uint16')\n",
    "    \n",
    "    return sequences, statics, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_x_train, static_x_train, seq_y_train = get_sequence_sets(X_train)\n",
    "seq_x_test, static_x_test, seq_y_test = get_sequence_sets(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(88133, 29)\n",
      "(88133, 17)\n",
      "(88133, 29)\n",
      "(9777, 29)\n",
      "(9777, 17)\n",
      "(9777, 29)\n"
     ]
    }
   ],
   "source": [
    "print(seq_x_train.shape)\n",
    "print(static_x_train.shape)\n",
    "print(seq_y_train.shape)\n",
    "\n",
    "print(seq_x_test.shape)\n",
    "print(static_x_test.shape)\n",
    "print(seq_y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(batch_size=600, sequence_length=(TOKENS_BEFORE_PREDICTION - TOKENS_PREDICTED)):\n",
    "    static_input = Input(batch_shape=(batch_size, len(STATIC_COLUMNS),))\n",
    "    seq_input = Input(batch_shape=(batch_size, sequence_length))\n",
    "\n",
    "    sequential_model = Embedding(\n",
    "        VOCABULARY_SIZE, \n",
    "        GLOVE_SIZE, \n",
    "        weights=[EMBEDDING_INIT], \n",
    "        input_length=sequence_length,\n",
    "        trainable=False,\n",
    "        name='sequence_embedding',\n",
    "    )(seq_input)\n",
    "\n",
    "    sequential_model = (LSTM(1000, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'))(sequential_model)\n",
    "    sequential_model = (LSTM(500, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'))(sequential_model)\n",
    "\n",
    "\n",
    "    sequential_model = Model(inputs=seq_input, outputs=sequential_model)\n",
    "\n",
    "\n",
    "    static_model = Dense(100, activation='tanh')(static_input)\n",
    "    static_model = Dropout(0.1)(static_model)\n",
    "    static_model = RepeatVector(sequence_length)(static_model)\n",
    "\n",
    "    static_model = Model(inputs=static_input, outputs=static_model)\n",
    "\n",
    "    combined = tf.keras.layers.concatenate(sequential_model.outputs + static_model.outputs, axis=2)\n",
    "\n",
    "    final_model = Dense(VOCABULARY_SIZE // 5)(combined)\n",
    "    final_model = Dense(VOCABULARY_SIZE)(combined)\n",
    "\n",
    "    return Model(inputs=sequential_model.inputs + static_model.inputs, outputs=final_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 700\n",
    "\n",
    "TRAIN_NEW_BATCHED_SIZE = (seq_x_train.shape[0] - (seq_x_train.shape[0] % BATCH_SIZE))\n",
    "TEST_NEW_BATCHED_SIZE = (seq_x_test.shape[0] - (seq_x_test.shape[0] % BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_v2 = create_model(batch_size=BATCH_SIZE)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(700, 29)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            [(700, 17)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequence_embedding (Embedding)  (700, 29, 200)       4265200     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (700, 100)           1800        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     (700, 29, 1000)      4804000     sequence_embedding[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (700, 100)           0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (700, 29, 500)       3002000     lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector (RepeatVector)    (700, 29, 100)       0           dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (700, 29, 600)       0           lstm_1[0][0]                     \n",
      "                                                                 repeat_vector[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (700, 29, 21326)     12816926    concatenate[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 24,889,926\n",
      "Trainable params: 20,624,726\n",
      "Non-trainable params: 4,265,200\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_v2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "model_v2.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = date.today().strftime('%Y.%m.%d.') + str(int(time.time()))\n",
    "\n",
    "csv_logger = CSVLogger('./Documents/scripts/logs/train.%s.csv' % now, append=True)\n",
    "\n",
    "es = EarlyStopping(mode='min', patience=15, monitor='loss')\n",
    "\n",
    "mc = ModelCheckpoint(\n",
    "    './Documents/scripts/models/best_model.%s.weights' % now, \n",
    "    mode='min', \n",
    "    save_best_only=True,\n",
    "    save_weights_only=True,\n",
    "    monitor='loss'\n",
    ")\n",
    "\n",
    "lrr = ReduceLROnPlateau(factor=0.2, min_lr=1e-4, patience=10, monitor='loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2020.06.07.1591567376'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 87500 samples\n",
      "Epoch 1/80\n",
      "87500/87500 [==============================] - 80s 918us/sample - loss: 5.4515\n",
      "Epoch 2/80\n",
      "87500/87500 [==============================] - 79s 899us/sample - loss: 5.2086\n",
      "Epoch 3/80\n",
      "87500/87500 [==============================] - 79s 898us/sample - loss: 5.0231\n",
      "Epoch 4/80\n",
      "87500/87500 [==============================] - 79s 899us/sample - loss: 4.8777\n",
      "Epoch 5/80\n",
      "87500/87500 [==============================] - 79s 898us/sample - loss: 4.7549\n",
      "Epoch 6/80\n",
      "87500/87500 [==============================] - 79s 898us/sample - loss: 4.6534\n",
      "Epoch 7/80\n",
      "87500/87500 [==============================] - 79s 903us/sample - loss: 4.5617\n",
      "Epoch 8/80\n",
      "87500/87500 [==============================] - 79s 901us/sample - loss: 4.4835\n",
      "Epoch 9/80\n",
      "87500/87500 [==============================] - 79s 900us/sample - loss: 4.4165\n",
      "Epoch 10/80\n",
      "87500/87500 [==============================] - 79s 901us/sample - loss: 4.3543\n",
      "Epoch 11/80\n",
      "87500/87500 [==============================] - 79s 901us/sample - loss: 4.2980\n",
      "Epoch 12/80\n",
      "87500/87500 [==============================] - 79s 903us/sample - loss: 4.2458\n",
      "Epoch 13/80\n",
      "87500/87500 [==============================] - 79s 903us/sample - loss: 4.1982\n",
      "Epoch 14/80\n",
      "87500/87500 [==============================] - 79s 901us/sample - loss: 4.1541\n",
      "Epoch 15/80\n",
      "87500/87500 [==============================] - 79s 902us/sample - loss: 4.1125\n",
      "Epoch 16/80\n",
      "87500/87500 [==============================] - 79s 902us/sample - loss: 4.0728\n",
      "Epoch 17/80\n",
      "87500/87500 [==============================] - 79s 900us/sample - loss: 4.0380\n",
      "Epoch 18/80\n",
      "87500/87500 [==============================] - 79s 901us/sample - loss: 4.0010\n",
      "Epoch 19/80\n",
      "87500/87500 [==============================] - 79s 901us/sample - loss: 3.9684\n",
      "Epoch 20/80\n",
      "87500/87500 [==============================] - 79s 900us/sample - loss: 3.9371\n",
      "Epoch 21/80\n",
      "87500/87500 [==============================] - 79s 902us/sample - loss: 3.9089\n",
      "Epoch 22/80\n",
      "87500/87500 [==============================] - 79s 901us/sample - loss: 3.8809\n",
      "Epoch 23/80\n",
      "87500/87500 [==============================] - 79s 902us/sample - loss: 3.8523\n",
      "Epoch 24/80\n",
      "87500/87500 [==============================] - 79s 902us/sample - loss: 3.8270\n",
      "Epoch 25/80\n",
      "87500/87500 [==============================] - 79s 902us/sample - loss: 3.8024\n",
      "Epoch 26/80\n",
      "87500/87500 [==============================] - 79s 900us/sample - loss: 3.7800\n",
      "Epoch 27/80\n",
      "87500/87500 [==============================] - 79s 901us/sample - loss: 3.7567\n",
      "Epoch 28/80\n",
      "87500/87500 [==============================] - 79s 900us/sample - loss: 3.7352\n",
      "Epoch 29/80\n",
      "87500/87500 [==============================] - 79s 900us/sample - loss: 3.7117\n",
      "Epoch 30/80\n",
      "87500/87500 [==============================] - 79s 900us/sample - loss: 3.6927\n",
      "Epoch 31/80\n",
      "87500/87500 [==============================] - 79s 902us/sample - loss: 3.6717\n",
      "Epoch 32/80\n",
      "87500/87500 [==============================] - 79s 900us/sample - loss: 3.6514\n",
      "Epoch 33/80\n",
      "87500/87500 [==============================] - 79s 901us/sample - loss: 3.6335\n",
      "Epoch 34/80\n",
      "87500/87500 [==============================] - 79s 899us/sample - loss: 3.6141\n",
      "Epoch 35/80\n",
      "87500/87500 [==============================] - 79s 900us/sample - loss: 3.5958\n",
      "Epoch 36/80\n",
      "87500/87500 [==============================] - 79s 899us/sample - loss: 3.5801\n",
      "Epoch 37/80\n",
      "87500/87500 [==============================] - 79s 902us/sample - loss: 3.5618\n",
      "Epoch 38/80\n",
      "87500/87500 [==============================] - 79s 901us/sample - loss: 3.5454\n",
      "Epoch 39/80\n",
      "87500/87500 [==============================] - 79s 901us/sample - loss: 3.5272\n",
      "Epoch 40/80\n",
      "87500/87500 [==============================] - 79s 901us/sample - loss: 3.5129\n",
      "Epoch 41/80\n",
      "87500/87500 [==============================] - 79s 902us/sample - loss: 3.4969\n",
      "Epoch 42/80\n",
      "87500/87500 [==============================] - 79s 902us/sample - loss: 3.4802\n",
      "Epoch 43/80\n",
      "87500/87500 [==============================] - 79s 902us/sample - loss: 3.4669\n",
      "Epoch 44/80\n",
      "87500/87500 [==============================] - 79s 902us/sample - loss: 3.4519\n",
      "Epoch 45/80\n",
      "87500/87500 [==============================] - 79s 899us/sample - loss: 3.4364\n",
      "Epoch 46/80\n",
      "87500/87500 [==============================] - 79s 902us/sample - loss: 3.4218\n",
      "Epoch 47/80\n",
      "87500/87500 [==============================] - 79s 902us/sample - loss: 3.4089\n",
      "Epoch 48/80\n",
      "87500/87500 [==============================] - 79s 900us/sample - loss: 3.3950\n",
      "Epoch 49/80\n",
      "87500/87500 [==============================] - 79s 902us/sample - loss: 3.3819\n",
      "Epoch 50/80\n",
      "87500/87500 [==============================] - 79s 902us/sample - loss: 3.3678\n",
      "Epoch 51/80\n",
      "87500/87500 [==============================] - 79s 902us/sample - loss: 3.3540\n",
      "Epoch 52/80\n",
      "87500/87500 [==============================] - 79s 901us/sample - loss: 3.3406\n",
      "Epoch 53/80\n",
      "87500/87500 [==============================] - 79s 902us/sample - loss: 3.3304\n",
      "Epoch 54/80\n",
      "87500/87500 [==============================] - 79s 903us/sample - loss: 3.3162\n",
      "Epoch 55/80\n",
      "87500/87500 [==============================] - 79s 903us/sample - loss: 3.3045\n",
      "Epoch 56/80\n",
      "87500/87500 [==============================] - 79s 904us/sample - loss: 3.2927\n",
      "Epoch 57/80\n",
      "87500/87500 [==============================] - 79s 904us/sample - loss: 3.2803\n",
      "Epoch 58/80\n",
      "87500/87500 [==============================] - 79s 904us/sample - loss: 3.2696\n",
      "Epoch 59/80\n",
      "87500/87500 [==============================] - 79s 904us/sample - loss: 3.2593\n",
      "Epoch 60/80\n",
      "87500/87500 [==============================] - 79s 904us/sample - loss: 3.2475\n",
      "Epoch 61/80\n",
      "87500/87500 [==============================] - 79s 904us/sample - loss: 3.2365\n",
      "Epoch 62/80\n",
      "87500/87500 [==============================] - 79s 903us/sample - loss: 3.2258\n",
      "Epoch 63/80\n",
      "87500/87500 [==============================] - 79s 903us/sample - loss: 3.2141\n",
      "Epoch 64/80\n",
      "87500/87500 [==============================] - 79s 903us/sample - loss: 3.2038\n",
      "Epoch 65/80\n",
      "87500/87500 [==============================] - 79s 903us/sample - loss: 3.1931\n",
      "Epoch 66/80\n",
      "87500/87500 [==============================] - 79s 902us/sample - loss: 3.1840\n",
      "Epoch 67/80\n",
      "87500/87500 [==============================] - 79s 903us/sample - loss: 3.1733\n",
      "Epoch 68/80\n",
      "87500/87500 [==============================] - 79s 902us/sample - loss: 3.1633\n",
      "Epoch 69/80\n",
      "87500/87500 [==============================] - 79s 902us/sample - loss: 3.1541\n",
      "Epoch 70/80\n",
      "87500/87500 [==============================] - 79s 901us/sample - loss: 3.1438\n",
      "Epoch 71/80\n",
      "87500/87500 [==============================] - 79s 901us/sample - loss: 3.1333\n",
      "Epoch 72/80\n",
      "87500/87500 [==============================] - 79s 902us/sample - loss: 3.1244\n",
      "Epoch 73/80\n",
      "87500/87500 [==============================] - 79s 900us/sample - loss: 3.1141\n",
      "Epoch 74/80\n",
      "87500/87500 [==============================] - 79s 901us/sample - loss: 3.1046\n",
      "Epoch 75/80\n",
      "87500/87500 [==============================] - 79s 904us/sample - loss: 3.0960\n",
      "Epoch 76/80\n",
      "87500/87500 [==============================] - 79s 902us/sample - loss: 3.0862\n",
      "Epoch 77/80\n",
      "87500/87500 [==============================] - 79s 902us/sample - loss: 3.0766\n",
      "Epoch 78/80\n",
      "87500/87500 [==============================] - 79s 901us/sample - loss: 3.0682\n",
      "Epoch 79/80\n",
      "87500/87500 [==============================] - 79s 901us/sample - loss: 3.0597\n",
      "Epoch 80/80\n",
      "87500/87500 [==============================] - 79s 902us/sample - loss: 3.0517\n"
     ]
    }
   ],
   "source": [
    "history = model_v2.fit(\n",
    "    (seq_x_train[:TRAIN_NEW_BATCHED_SIZE], static_x_train[:TRAIN_NEW_BATCHED_SIZE]),\n",
    "    seq_y_train[:TRAIN_NEW_BATCHED_SIZE],\n",
    "    epochs = 80,\n",
    "    batch_size = BATCH_SIZE,\n",
    "#     validation_data=(\n",
    "#         (seq_x_test[:TEST_NEW_BATCHED_SIZE], static_x_test[:TEST_NEW_BATCHED_SIZE]), \n",
    "#         seq_y_test[:TEST_NEW_BATCHED_SIZE]\n",
    "#     ),\n",
    "     callbacks=[csv_logger, es, mc, lrr],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(batch_size=1)\n",
    "model.load_weights('./Documents/scripts/models/best_model.2020.06.07.1591567376.weights')\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_8\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            [(1, 29)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            [(1, 17)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequence_embedding (Embedding)  (1, 29, 200)         4265200     input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (1, 100)             1800        input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                   (1, 29, 1000)        4804000     sequence_embedding[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (1, 100)             0           dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_5 (LSTM)                   (1, 29, 500)         3002000     lstm_4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_2 (RepeatVector)  (1, 29, 100)         0           dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (1, 29, 600)         0           lstm_5[0][0]                     \n",
      "                                                                 repeat_vector_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (1, 29, 21326)       12816926    concatenate_2[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 24,889,926\n",
      "Trainable params: 20,624,726\n",
      "Non-trainable params: 4,265,200\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "STAR_SIGN_NAMES = df[['star_sign_index', 'star_sign_name']].drop_duplicates().to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def continue_text(model, text_sequence, predicted_size, star_sign, year, month, day):\n",
    "    if isinstance(star_sign, str):\n",
    "        star_sign = list(filter(lambda x: x['star_sign_name'].lower() == star_sign.lower(), STAR_SIGN_NAMES))[0]['star_sign_index']\n",
    "    \n",
    "    tf.keras.backend.set_floatx('float64')\n",
    "    \n",
    "    tokenized_text = tokenizer.texts_to_sequences([text_sequence])\n",
    "    predicted_size = predicted_size + len(tokenized_text[0])\n",
    "    \n",
    "    year = (year - year_mean) / year_std\n",
    "    cos_mon = np.cos(np.pi * 2 * month / 12.0)\n",
    "    sin_mon = np.cos(np.pi * 2 * month / 12.0)\n",
    "    cos_day = np.cos(np.pi * 2 * day / 31.0)\n",
    "    sin_day = np.cos(np.pi * 2 * day / 31.0)\n",
    "    \n",
    "    star_signs = np.zeros(12)\n",
    "    star_signs[star_sign] = 1\n",
    "    \n",
    "    static_feats = np.array([year, cos_mon, sin_mon, cos_day, sin_day])\n",
    "    static_feats = np.concatenate([static_feats, star_signs])\n",
    "    \n",
    "    model.reset_states()\n",
    "    temperature = 1\n",
    "    \n",
    "    tf.keras.backend.set_floatx('float64')\n",
    "    \n",
    "    while len(tokenized_text[0]) < predicted_size:\n",
    "        padded_sentence = pad_sequences(\n",
    "            tokenized_text[-(TOKENS_BEFORE_PREDICTION - TOKENS_PREDICTED):], \n",
    "            maxlen=(TOKENS_BEFORE_PREDICTION - TOKENS_PREDICTED)\n",
    "        )\n",
    "        \n",
    "        op = model([np.asarray(padded_sentence).reshape(1, -1), static_feats.reshape(1, -1)])\n",
    "        predictions = tf.squeeze(op, 0)\n",
    "        \n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
    "        \n",
    "        tokenized_text[0].append(predicted_id)\n",
    "      \n",
    "    return ' '.join(map(lambda x: dict(map(reversed, tokenizer.word_index.items()))[x], tokenized_text[0]))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'with the product of their materials materials create an amazing world of how much other information could be a tedious exercise be slightly emotionally you have a very useful period'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "continue_text(model, \"\", 30, 'Taurus', 2019, 1, 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'today rattles oneonone hosted streamlining hosted hosted streamlining hosted hosted invincibility authoritarianism hosted allconsuming jail hosted hosted manuscript invincibility oven wednesday'"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "continue_text(model, \"Today\", 20, 'Aries', 2019, 1, 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./Documents/scripts/dictionary.2020.05.03.1.json', 'w') as f:\n",
    "    f.write(json.dumps(list(tokenizer.word_index.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
